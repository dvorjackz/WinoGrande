{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_lg\n",
    "import pprint\n",
    "import collections\n",
    "\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import MiniGCDataset\n",
    "import dgl.function as fn\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parser = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_n_e(nodes, edges):\n",
    "\n",
    "    num_nodes = len(nodes)\n",
    "    new_edges = []\n",
    "    for e1, e2 in edges:\n",
    "        new_edges.append( [nodes[e1], nodes[e2]] ) \n",
    "    return num_nodes, new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding_for_dp_token(token, bert_tokens, bert_embeddings):\n",
    "    try:\n",
    "        idx = bert_tokens.index(token)\n",
    "        return bert_embeddings[idx]\n",
    "    except ValueError:\n",
    "        temp = token\n",
    "        start, end = 0, 0\n",
    "        seq = False\n",
    "        for i, bert_token in enumerate(bert_tokens):\n",
    "            if bert_token in temp:\n",
    "                temp = temp[len(bert_token):]\n",
    "                if not seq:\n",
    "                    start = i\n",
    "                    seq = True\n",
    "            else:\n",
    "                temp = token\n",
    "                seq = False\n",
    "            if len(temp) == 0:\n",
    "                end = i + 1\n",
    "                break\n",
    "\n",
    "        bert_emb_tensor = bert_embeddings[start:end]\n",
    "        return torch.mean(bert_emb_tensor, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'xs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = torch.load(\"X_{}.pt\".format(size))\n",
    "y_data = torch.load(\"y_{}.pt\".format(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_target(dp_token, options, debug=False):\n",
    "    \n",
    "    if dp_token == options[0].split(' ')[0]:\n",
    "        return True\n",
    "    \n",
    "    if dp_token == options[1].split(' ')[0]:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 13]\n",
      "[0, 4, 13]\n",
      "[0, 4, 13]\n",
      "[0, 4, 13]\n",
      "[5, 13, 16]\n",
      "[5, 13, 16]\n",
      "[5, 13, 16]\n",
      "[5, 13, 16]\n",
      "[0, 10, 15]\n",
      "[0, 10, 15]\n",
      "[0, 10, 15]\n",
      "[0, 10, 15]\n",
      "[1, 3, 14]\n",
      "[1, 3, 14]\n",
      "[1, 3, 14]\n",
      "[1, 3, 14]\n",
      "[0, 5, 11]\n",
      "[0, 5, 11]\n",
      "[0, 5, 11]\n",
      "[0, 5, 11]\n",
      "[10, 16, 19]\n",
      "[10, 16, 19]\n",
      "[10, 16, 19]\n",
      "[10, 16, 19]\n",
      "[9, 12, 16]\n",
      "[9, 12, 16]\n",
      "[9, 12, 16]\n",
      "[9, 12, 16]\n",
      "[10, 16, 21]\n",
      "[10, 16, 21]\n",
      "[10, 16, 21]\n",
      "[10, 16, 21]\n",
      "[13, 18, 22]\n",
      "[13, 18, 22]\n",
      "[13, 18, 22]\n",
      "[13, 18, 22]\n",
      "[0, 7, 9]\n",
      "[0, 7, 9]\n",
      "[0, 7, 9]\n",
      "[0, 7, 9]\n",
      "[6, 8, 23]\n",
      "[6, 8, 23]\n",
      "[6, 8, 23]\n",
      "[6, 8, 23]\n",
      "[0, 9, 13]\n",
      "[0, 9, 13]\n",
      "[0, 9, 13]\n",
      "[0, 9, 13]\n",
      "[0, 5, 13]\n",
      "[0, 5, 13]\n",
      "[0, 5, 13]\n",
      "[0, 5, 13]\n",
      "[1, 9, 12]\n",
      "[1, 9, 12]\n",
      "[1, 9, 12]\n",
      "[1, 9, 12]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[1, 9, 13]\n",
      "[1, 9, 13]\n",
      "[1, 9, 13]\n",
      "[1, 9, 13]\n",
      "[4, 9, 24]\n",
      "[4, 9, 24]\n",
      "[4, 9, 24]\n",
      "[4, 9, 24]\n",
      "[2, 5, 14]\n",
      "[2, 5, 14]\n",
      "[2, 5, 14]\n",
      "[2, 5, 14]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[0, 4, 15]\n",
      "[0, 4, 15]\n",
      "[0, 4, 15]\n",
      "[0, 4, 15]\n",
      "[4, 6, 14]\n",
      "[4, 6, 14]\n",
      "[4, 6, 14]\n",
      "[4, 6, 14]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 12, 18]\n",
      "[0, 12, 18]\n",
      "[0, 12, 18]\n",
      "[0, 12, 18]\n",
      "[0, 2, 12]\n",
      "[0, 2, 12]\n",
      "[0, 2, 12]\n",
      "[0, 2, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[5, 15, 18]\n",
      "[5, 15, 18]\n",
      "[5, 15, 18]\n",
      "[5, 15, 18]\n",
      "[5, 10, 20]\n",
      "[5, 10, 20]\n",
      "[5, 10, 20]\n",
      "[5, 10, 20]\n",
      "[7, 12, 14]\n",
      "[7, 12, 14]\n",
      "[7, 12, 14]\n",
      "[7, 12, 14]\n",
      "[0, 4, 18]\n",
      "[0, 4, 18]\n",
      "[0, 4, 17]\n",
      "[0, 4, 17]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 4, 16]\n",
      "[0, 4, 16]\n",
      "[0, 4, 16]\n",
      "[0, 4, 16]\n",
      "[4, 9, 22]\n",
      "[4, 9, 22]\n",
      "[4, 9, 22]\n",
      "[4, 9, 22]\n",
      "[4, 10, 12]\n",
      "[4, 10, 12]\n",
      "[4, 10, 12]\n",
      "[4, 10, 12]\n",
      "[2, 5, 10]\n",
      "[2, 5, 10]\n",
      "[4, 6, 11]\n",
      "[4, 6, 11]\n",
      "[4, 10, 16]\n",
      "[4, 10, 16]\n",
      "[4, 10, 16]\n",
      "[4, 10, 16]\n",
      "[10, 13, 16]\n",
      "[10, 13, 16]\n",
      "[10, 13, 16]\n",
      "[10, 13, 16]\n",
      "[0, 7, 11]\n",
      "[0, 7, 11]\n",
      "[0, 7, 11]\n",
      "[0, 7, 11]\n",
      "[0, 16, 22]\n",
      "[0, 16, 22]\n",
      "[0, 16, 22]\n",
      "[0, 16, 22]\n",
      "[0, 7, 10]\n",
      "[0, 7, 10]\n",
      "[0, 7, 10]\n",
      "[0, 7, 10]\n",
      "[0, 9, 15]\n",
      "[0, 9, 15]\n",
      "[0, 9, 15]\n",
      "[0, 9, 15]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 7, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n",
      "[0, 8, 11]\n",
      "[0, 8, 11]\n",
      "[0, 8, 11]\n",
      "[0, 8, 11]\n",
      "[0, 3, 13]\n",
      "[0, 3, 13]\n",
      "[0, 3, 13]\n",
      "[0, 3, 13]\n",
      "[3, 10, 13]\n",
      "[3, 10, 13]\n",
      "[3, 10, 13]\n",
      "[3, 10, 13]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[3, 9, 17]\n",
      "[3, 9, 17]\n",
      "[3, 9, 17]\n",
      "[3, 9, 17]\n",
      "[3, 7, 12]\n",
      "[3, 7, 12]\n",
      "[3, 7, 12]\n",
      "[3, 7, 12]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[0, 12, 15]\n",
      "[5, 12, 17]\n",
      "[5, 12, 17]\n",
      "[5, 12, 18]\n",
      "[5, 12, 18]\n",
      "[5, 10, 14]\n",
      "[5, 10, 14]\n",
      "[5, 10, 14]\n",
      "[5, 10, 14]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 9, 12]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[4, 10, 13]\n",
      "[4, 10, 13]\n",
      "[4, 10, 13]\n",
      "[4, 10, 13]\n",
      "[2, 4, 19]\n",
      "[2, 4, 19]\n",
      "[2, 4, 19]\n",
      "[2, 4, 19]\n",
      "[0, 6, 8]\n",
      "[0, 6, 8]\n",
      "[0, 6, 8]\n",
      "[0, 6, 8]\n",
      "[8, 16, 18]\n",
      "[8, 16, 18]\n",
      "[8, 16, 18]\n",
      "[8, 16, 18]\n",
      "[0, 6, 12]\n",
      "[0, 6, 12]\n",
      "[0, 6, 12]\n",
      "[0, 6, 12]\n",
      "[1, 9, 11]\n",
      "[1, 9, 11]\n",
      "[1, 9, 11]\n",
      "[1, 9, 11]\n",
      "[0, 2, 15]\n",
      "[0, 2, 15]\n",
      "[0, 2, 14]\n",
      "[0, 2, 14]\n",
      "[0, 9, 18]\n",
      "[0, 9, 18]\n",
      "[0, 9, 18]\n",
      "[0, 9, 18]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 3, 12]\n",
      "[0, 14, 21]\n",
      "[0, 14, 21]\n",
      "[0, 14, 21]\n",
      "[0, 14, 21]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[0, 10, 14]\n",
      "[0, 9, 11]\n",
      "[0, 9, 11]\n",
      "[0, 9, 11]\n",
      "[0, 9, 11]\n",
      "[7, 16, 20]\n",
      "[7, 16, 20]\n",
      "[7, 16, 20]\n",
      "[7, 16, 20]\n",
      "[2, 9, 12]\n",
      "[2, 9, 12]\n",
      "[2, 9, 12]\n",
      "[2, 9, 12]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[7, 11, 15]\n",
      "[9, 13, 20]\n",
      "[9, 13, 20]\n",
      "[9, 13, 20]\n",
      "[9, 13, 20]\n",
      "[0, 4, 14]\n",
      "[0, 4, 14]\n",
      "[0, 4, 14]\n",
      "[0, 4, 14]\n",
      "[11, 13, 15]\n",
      "[11, 13, 15]\n",
      "[11, 13, 15]\n",
      "[11, 13, 15]\n",
      "[5, 12, 16]\n",
      "[5, 12, 16]\n",
      "[5, 12, 16]\n",
      "[5, 12, 16]\n",
      "[1, 8, 14]\n",
      "[1, 8, 14]\n",
      "[1, 8, 14]\n",
      "[1, 8, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[0, 11, 14]\n",
      "[6, 8, 16]\n",
      "[6, 8, 16]\n",
      "[6, 8, 16]\n",
      "[6, 8, 16]\n",
      "[7, 11, 17]\n",
      "[7, 11, 17]\n",
      "[7, 11, 17]\n",
      "[7, 11, 17]\n",
      "[4, 10, 14]\n",
      "[4, 10, 14]\n",
      "[4, 10, 14]\n",
      "[4, 10, 14]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n",
      "[6, 9, 13]\n"
     ]
    }
   ],
   "source": [
    "all_graphs = []\n",
    "gcn_offsets = []\n",
    "cls_tokens = []\n",
    "for row in X_preprocessed:\n",
    "    sentence = row['sentence']\n",
    "    bert_embeddings = row['encoding'][0]\n",
    "    bert_tokens = row['tokens']\n",
    "    options = row['options']\n",
    "    \n",
    "    doc = parser(sentence)\n",
    "    nodes = collections.OrderedDict()\n",
    "    edges = []\n",
    "    edge_type = []\n",
    "    \n",
    "    offsets = []\n",
    "    offset_words = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # skip words that aren't targets or separated by one edge from target\n",
    "        if not (is_target(token.text, options) or is_target(token.head.text, options)):\n",
    "            continue\n",
    "        \n",
    "        if token.i not in nodes:\n",
    "            nodes[token.i] = len(nodes)\n",
    "            edges.append( [token.i, token.i])\n",
    "            edge_type.append(0)\n",
    "        \n",
    "        if token.head.i not in nodes:\n",
    "            nodes[token.head.i] = len(nodes)\n",
    "            edges.append( [token.head.i, token.head.i] )\n",
    "            edge_type.append(0)\n",
    "            \n",
    "        if token.dep_ != 'ROOT':\n",
    "            edges.append( [ token.head.i, token.i ])\n",
    "            edge_type.append(1)\n",
    "            edges.append( [ token.i, token.head.i ])\n",
    "            edge_type.append(2)\n",
    "            \n",
    "        if is_target(token.text, options):\n",
    "            offsets.append(token.i)\n",
    "            offset_words.append(token.text)\n",
    "    \n",
    "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
    "    print(offsets)\n",
    "    \n",
    "    if (len(offsets) != 3):\n",
    "        print(\"UNEXPECTED: at least 3 positions should be in offsets\")\n",
    "        print(sentence, options, len(offsets))\n",
    "        print(offset_words)\n",
    "    \n",
    "    gcn_offset = [nodes[offset] for offset in offsets]\n",
    "    gcn_offsets.append(gcn_offset)\n",
    "    \n",
    "    G = dgl.DGLGraph()\n",
    "    G.add_nodes(num_nodes)\n",
    "    G.add_edges(list(zip(*tran_edges))[0], list(zip(*tran_edges))[1])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not (is_target(token.text, options) or is_target(token.head.text, options)):\n",
    "            continue\n",
    "            \n",
    "        dp_token = token.text\n",
    "        embedding = bert_embedding_for_dp_token(dp_token, bert_tokens, bert_embeddings)\n",
    "        G.nodes[ nodes[token.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "        \n",
    "        head_dp_token = token.head.text\n",
    "        embedding = bert_embedding_for_dp_token(head_dp_token, bert_tokens, bert_embeddings)\n",
    "        G.nodes[ nodes[token.head.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "        \n",
    "    edge_norm = []\n",
    "    for e1, e2 in tran_edges:\n",
    "        if e1 == e2:\n",
    "            edge_norm.append(1)\n",
    "        else:\n",
    "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "    edge_type = torch.from_numpy(np.array(edge_type))\n",
    "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float()\n",
    "    \n",
    "    G.edata.update({'rel_type': edge_type,})\n",
    "    G.edata.update({'norm': edge_norm})\n",
    "    # todo: Add <s> token embedding to graph here.\n",
    "    all_graphs.append(G)\n",
    "    cls_tokens.append(bert_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Save all_graphs, gcn_offsets & cls_tokens\n",
    "# # cls_tokens = torch.tensor(cls_tokens)\n",
    "# cls_tokens = torch.stack(cls_tokens)\n",
    "# gcn_offsets = torch.tensor(gcn_offsets)\n",
    "\n",
    "# # https://docs.dgl.ai/en/0.4.x/generated/dgl.data.utils.load_graphs.html\n",
    "# save_graphs(\"data/X_train_graphs_{}.bin\".format(size), all_graphs) \n",
    "# torch.save(cls_tokens, \"data/X_train_cls_tokens_{}.bin\".format(size))\n",
    "# torch.save(gcn_offsets, \"data/X_train_gcn_offsets_{}.bin\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK OVER - ALL CODE BELOW IS SIMPLY BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_embedding_for_dp_token(\"bread\", bt, be))\n",
    "# print(torch.mean(torch.stack(be[2:5]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = collections.OrderedDict()\n",
    "edges = []\n",
    "edge_type = []\n",
    "\n",
    "#     for i_word, word in enumerate(parse_rst['tokens']):\n",
    "#         # TODO: skip words that aren't targets or seperated by one edge from target\n",
    "    \n",
    "#         if i_word not in nodes:\n",
    "#             nodes[i_word] = len(nodes)\n",
    "#             edges.append( [i_word, i_word])\n",
    "#             edge_type.append(0)\n",
    "#         if word['head'] not in nodes:\n",
    "#             nodes[word['head']] = len(nodes)\n",
    "#             edges.append( [ word['head'], word['head'] ] )\n",
    "#             edge_type.append(0)\n",
    "\n",
    "#         if word['dep'] != 'ROOT':\n",
    "#             edges.append([ word['head'], word['id'] ])\n",
    "#             edge_type.append(1)\n",
    "#             edges.append([ word['id'], word['head'] ])\n",
    "#             edge_type.append(2)\n",
    "    \n",
    "num_nodes, tran_edges = transfer_n_e(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph()\n",
    "G.add_nodes(num_nodes)\n",
    "G.add_edges(list(zip(*tran_edges))[0], list(zip(*tran_edges))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    dp_token = token.text\n",
    "    embedding = bert_embedding_for_dp_token(dp_token, bt, be)\n",
    "    G.nodes[ nodes[token.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "    \n",
    "    head_dp_token = token.head.text\n",
    "    embedding = bert_embedding_for_dp_token(head_dp_token, bt, be)\n",
    "    G.nodes[ nodes[token.head.i] ].data['h'] = embedding.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_norm = []\n",
    "for e1, e2 in tran_edges:\n",
    "    if e1 == e2:\n",
    "        edge_norm.append(1)\n",
    "    else:\n",
    "        edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "edge_type = torch.from_numpy(np.array(edge_type))\n",
    "edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edata.update({'rel_type': edge_type,})\n",
    "G.edata.update({'norm': edge_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: repeat above steps in a loop for all input!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
