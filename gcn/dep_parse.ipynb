{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_lg\n",
    "import pprint\n",
    "import collections\n",
    "\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import MiniGCDataset\n",
    "import dgl.function as fn\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parser = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_n_e(nodes, edges):\n",
    "\n",
    "    num_nodes = len(nodes)\n",
    "    new_edges = []\n",
    "    for e1, e2 in edges:\n",
    "        new_edges.append( [nodes[e1], nodes[e2]] ) \n",
    "    return num_nodes, new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding_for_dp_token(token, bert_tokens, bert_embeddings, debug=False):\n",
    "    try:\n",
    "        idx = bert_tokens.index(token)\n",
    "        return bert_embeddings[idx]\n",
    "    except ValueError:\n",
    "        temp = token\n",
    "        start, end = 0, 0\n",
    "        seq = False\n",
    "        for i, bert_token in enumerate(bert_tokens):\n",
    "            if debug:\n",
    "                print(\"HA:\", bert_token, token)\n",
    "            if bert_token in temp:\n",
    "                temp = temp[len(bert_token):]\n",
    "                if not seq:\n",
    "                    start = i\n",
    "                    seq = True\n",
    "            else:\n",
    "                temp = token\n",
    "                seq = False\n",
    "            if len(temp) == 0:\n",
    "                end = i + 1\n",
    "                break\n",
    "        \n",
    "        if (debug):\n",
    "            print(start, end)\n",
    "\n",
    "        bert_emb_tensor = bert_embeddings[start:end]\n",
    "        return torch.mean(bert_emb_tensor, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'xs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = torch.load(\"X_{}.pt\".format(size))\n",
    "y_data = torch.load(\"y_{}.pt\".format(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_target(dp_token, options, debug=False):\n",
    "    \n",
    "    if dp_token == options[0].split(' ')[0]:\n",
    "        return True\n",
    "    \n",
    "    if dp_token == options[1].split(' ')[0]:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNEXPECTED: bert_embedding_for_dp_token returns NaN\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan]])\n",
      "18 19 did Emily asked her sister Sarah if she needed any tampons or pads from the store, even though Emily didn't because she had switched to using menstrual cups. ['<s>', 'Emily', 'asked', 'her', 'sister', 'Sarah', 'if', 'she', 'needed', 'any', 'tamp', 'ons', 'or', 'pads', 'from', 'the', 'store', ',', 'even', 'though', 'Emily', 'didn', \"'t\", 'because', 'she', 'had', 'switched', 'to', 'using', 'menstrual', 'cups', '.', '</s>']\n",
      "[Emily, asked, her, sister, Sarah, if, she, needed, any, tampons, or, pads, from, the, store, ,, even, though, Emily, did, n't, because, she, had, switched, to, using, menstrual, cups, .]\n",
      "UNEXPECTED: bert_embedding_for_dp_token returns NaN\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan]])\n",
      "18 19 did Emily asked her sister Sarah if she needed any tampons or pads from the store, even though Sarah didn't because she had switched to using menstrual cups. ['<s>', 'Emily', 'asked', 'her', 'sister', 'Sarah', 'if', 'she', 'needed', 'any', 'tamp', 'ons', 'or', 'pads', 'from', 'the', 'store', ',', 'even', 'though', 'Sarah', 'didn', \"'t\", 'because', 'she', 'had', 'switched', 'to', 'using', 'menstrual', 'cups', '.', '</s>']\n",
      "[Emily, asked, her, sister, Sarah, if, she, needed, any, tampons, or, pads, from, the, store, ,, even, though, Sarah, did, n't, because, she, had, switched, to, using, menstrual, cups, .]\n"
     ]
    }
   ],
   "source": [
    "all_graphs = []\n",
    "gcn_offsets = []\n",
    "cls_tokens = []\n",
    "for row in X_preprocessed:\n",
    "    sentence = row['sentence']\n",
    "    bert_embeddings = row['encoding'][0]\n",
    "    bert_tokens = row['tokens']\n",
    "    options = row['options']\n",
    "    \n",
    "    doc = parser(sentence)\n",
    "    nodes = collections.OrderedDict()\n",
    "    edges = []\n",
    "    edge_type = []\n",
    "    \n",
    "    offsets = []\n",
    "    offset_words = []\n",
    "    \n",
    "    spacy_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        spacy_tokens.append(token)\n",
    "        \n",
    "        # skip words that aren't targets or separated by one edge from target\n",
    "        if not (is_target(token.text, options) or is_target(token.head.text, options)):\n",
    "            continue\n",
    "        \n",
    "        if token.i not in nodes:\n",
    "            nodes[token.i] = len(nodes)\n",
    "            edges.append( [token.i, token.i])\n",
    "            edge_type.append(0)\n",
    "        \n",
    "        if token.head.i not in nodes:\n",
    "            nodes[token.head.i] = len(nodes)\n",
    "            edges.append( [token.head.i, token.head.i] )\n",
    "            edge_type.append(0)\n",
    "            \n",
    "        if token.dep_ != 'ROOT':\n",
    "            edges.append( [ token.head.i, token.i ])\n",
    "            edge_type.append(1)\n",
    "            edges.append( [ token.i, token.head.i ])\n",
    "            edge_type.append(2)\n",
    "            \n",
    "        if is_target(token.text, options):\n",
    "            offsets.append(token.i)\n",
    "            offset_words.append(token.text)\n",
    "    \n",
    "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
    "    \n",
    "    if (len(offsets) != 3):\n",
    "        print(\"UNEXPECTED: at least 3 positions should be in offsets\")\n",
    "        print(sentence, options, len(offsets))\n",
    "        print(offset_words)\n",
    "    \n",
    "    gcn_offset = [nodes[offset] for offset in offsets]\n",
    "    gcn_offsets.append(gcn_offset)\n",
    "    \n",
    "    G = dgl.DGLGraph()\n",
    "    G.add_nodes(num_nodes)\n",
    "    G.add_edges(list(zip(*tran_edges))[0], list(zip(*tran_edges))[1])\n",
    "    \n",
    "    for i in range(len(doc))\n",
    "        token = doc[i]\n",
    "        if not (is_target(token.text, options) or is_target(token.head.text, options)):\n",
    "            continue\n",
    "            \n",
    "        if token[i + 1] == \"n\\'t\":\n",
    "            token = token[:token.index(\"n\\'t\")+1]\n",
    "        elif token[i] == \"n\\'t\"\n",
    "            token = token[token.index(\"n\\'t\")+1:]\n",
    "            \n",
    "        dp_token = token.text\n",
    "        embedding = bert_embedding_for_dp_token(dp_token, bert_tokens, bert_embeddings)\n",
    "        if(torch.isnan(embedding.unsqueeze(0)).any()):\n",
    "            print(\"UNEXPECTED: bert_embedding_for_dp_token returns NaN\")\n",
    "            print(embedding.unsqueeze(0))\n",
    "            print(token.i, token.text, sentence, bert_tokens)\n",
    "            print(spacy_tokens)\n",
    "            G.nodes[ nodes[token.i] ].data['h'] = torch.randn(1024).unsqueeze(0)\n",
    "        else:\n",
    "            G.nodes[ nodes[token.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "        \n",
    "        head_dp_token = token.head.text\n",
    "        embedding = bert_embedding_for_dp_token(head_dp_token, bert_tokens, bert_embeddings)\n",
    "        if(torch.isnan(embedding.unsqueeze(0)).any()):\n",
    "            print(\"UNEXPECTED: bert_embedding_for_dp_token returns NaN\")\n",
    "            print(embedding.unsqueeze(0))\n",
    "            print(token.i, token.head.i, token.head.text, sentence, bert_tokens)\n",
    "            print(spacy_tokens)\n",
    "            G.nodes[ nodes[token.head.i] ].data['h'] = torch.randn(1024).unsqueeze(0)\n",
    "        else:\n",
    "            G.nodes[ nodes[token.head.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "        \n",
    "    edge_norm = []\n",
    "    for e1, e2 in tran_edges:\n",
    "        if e1 == e2:\n",
    "            edge_norm.append(1)\n",
    "        else:\n",
    "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "    edge_type = torch.from_numpy(np.array(edge_type))\n",
    "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float()\n",
    "    \n",
    "    G.edata.update({'rel_type': edge_type,})\n",
    "    G.edata.update({'norm': edge_norm})\n",
    "    # todo: Add <s> token embedding to graph here.\n",
    "    all_graphs.append(G)\n",
    "    cls_tokens.append(bert_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save all_graphs, gcn_offsets & cls_tokens\n",
    "# cls_tokens = torch.tensor(cls_tokens)\n",
    "cls_tokens = torch.stack(cls_tokens)\n",
    "gcn_offsets = torch.tensor(gcn_offsets)\n",
    "\n",
    "# https://docs.dgl.ai/en/0.4.x/generated/dgl.data.utils.load_graphs.html\n",
    "save_graphs(\"data/X_train_graphs_{}.bin\".format(size), all_graphs) \n",
    "torch.save(cls_tokens, \"data/X_train_cls_tokens_{}.bin\".format(size))\n",
    "torch.save(gcn_offsets, \"data/X_train_gcn_offsets_{}.bin\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK OVER - ALL CODE BELOW IS SIMPLY BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n",
    "\n",
    "> \n",
    "\n",
    ">\n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_embedding_for_dp_token(\"bread\", bt, be))\n",
    "# print(torch.mean(torch.stack(be[2:5]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = collections.OrderedDict()\n",
    "edges = []\n",
    "edge_type = []\n",
    "\n",
    "#     for i_word, word in enumerate(parse_rst['tokens']):\n",
    "#         # TODO: skip words that aren't targets or seperated by one edge from target\n",
    "    \n",
    "#         if i_word not in nodes:\n",
    "#             nodes[i_word] = len(nodes)\n",
    "#             edges.append( [i_word, i_word])\n",
    "#             edge_type.append(0)\n",
    "#         if word['head'] not in nodes:\n",
    "#             nodes[word['head']] = len(nodes)\n",
    "#             edges.append( [ word['head'], word['head'] ] )\n",
    "#             edge_type.append(0)\n",
    "\n",
    "#         if word['dep'] != 'ROOT':\n",
    "#             edges.append([ word['head'], word['id'] ])\n",
    "#             edge_type.append(1)\n",
    "#             edges.append([ word['id'], word['head'] ])\n",
    "#             edge_type.append(2)\n",
    "    \n",
    "num_nodes, tran_edges = transfer_n_e(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph()\n",
    "G.add_nodes(num_nodes)\n",
    "G.add_edges(list(zip(*tran_edges))[0], list(zip(*tran_edges))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    dp_token = token.text\n",
    "    embedding = bert_embedding_for_dp_token(dp_token, bt, be)\n",
    "    G.nodes[ nodes[token.i] ].data['h'] = embedding.unsqueeze(0)\n",
    "    \n",
    "    head_dp_token = token.head.text\n",
    "    embedding = bert_embedding_for_dp_token(head_dp_token, bt, be)\n",
    "    G.nodes[ nodes[token.head.i] ].data['h'] = embedding.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_norm = []\n",
    "for e1, e2 in tran_edges:\n",
    "    if e1 == e2:\n",
    "        edge_norm.append(1)\n",
    "    else:\n",
    "        edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
    "\n",
    "edge_type = torch.from_numpy(np.array(edge_type))\n",
    "edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edata.update({'rel_type': edge_type,})\n",
    "G.edata.update({'norm': edge_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: repeat above steps in a loop for all input!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
