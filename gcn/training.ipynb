{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import autograd\n",
    "\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import MiniGCDataset\n",
    "import dgl.function as fn\n",
    "from dgl.data.utils import load_graphs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'xs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens = torch.load('data/X_train_cls_tokens_{}.bin'.format(size))\n",
    "gcn_offsets = torch.load(\"data/X_train_gcn_offsets_{}.bin\".format(size))\n",
    "all_graphs, _ = load_graphs(\"data/X_train_graphs_{}.bin\".format(size))\n",
    "\n",
    "y_data = torch.load('data/y_{}.pt'.format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, feat_size, num_rels, activation=None, gated = True):\n",
    "        \n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.num_rels = num_rels\n",
    "        self.activation = activation\n",
    "        self.gated = gated\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_rels, self.feat_size, 256))\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,gain=nn.init.calculate_gain('relu'))\n",
    "        \n",
    "        if self.gated:\n",
    "            self.gate_weight = nn.Parameter(torch.Tensor(self.num_rels, self.feat_size, 1))\n",
    "            nn.init.xavier_uniform_(self.gate_weight,gain=nn.init.calculate_gain('sigmoid'))\n",
    "        \n",
    "    def forward(self, g):\n",
    "        \n",
    "        weight = self.weight\n",
    "        gate_weight = self.gate_weight\n",
    "        \n",
    "        def message_func(edges):\n",
    "            w = weight[edges.data['rel_type']]\n",
    "            msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "            msg = msg * edges.data['norm']\n",
    "            \n",
    "            if self.gated:\n",
    "                gate_w = gate_weight[edges.data['rel_type']]\n",
    "                gate = torch.bmm(edges.src['h'].unsqueeze(1), gate_w).squeeze().reshape(-1,1)\n",
    "                gate = torch.sigmoid(gate)\n",
    "                msg = msg * gate\n",
    "                \n",
    "            return {'msg': msg}\n",
    "    \n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            h = self.activation(h)\n",
    "            return {'h': h}\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Full RGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNModel(nn.Module):\n",
    "    def __init__(self, h_dim, num_rels, num_hidden_layers=1, gated = True):\n",
    "        super(RGCNModel, self).__init__()\n",
    "\n",
    "        self.h_dim = h_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.gated = gated\n",
    "        \n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "       \n",
    "    def build_model(self):        \n",
    "        self.layers = nn.ModuleList() \n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            rgcn_layer = RGCNLayer(self.h_dim, self.num_rels, activation=F.relu, gated = self.gated)\n",
    "            self.layers.append(rgcn_layer)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        for layer in self.layers:\n",
    "            layer(g) # todo: maybe g = layer(g)??\n",
    "        \n",
    "        rst_hidden = []\n",
    "        for sub_g in dgl.unbatch(g):\n",
    "            rst_hidden.append(  sub_g.ndata['h']   )\n",
    "        return rst_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the Main Model (R-GCN + FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, gcn_out_size: int, bert_out_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_out_size = bert_out_size\n",
    "        self.gcn_out_size = gcn_out_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_out_size + gcn_out_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_out_size + gcn_out_size * 3, 256),    \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2), # todo: make sure 2 is fine.\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, gcn_outputs, offsets_gcn, bert_embeddings):\n",
    "        \n",
    "        gcn_extracted_outputs = [gcn_outputs[i].unsqueeze(0).gather(1, offsets_gcn[i].unsqueeze(0).unsqueeze(2)\n",
    "                                       .expand(-1, -1, gcn_outputs[i].unsqueeze(0).size(2))).view(gcn_outputs[i].unsqueeze(0).size(0), -1) for i in range(len(gcn_outputs))]\n",
    "        \n",
    "        gcn_extracted_outputs = torch.stack(gcn_extracted_outputs, dim=0).squeeze()\n",
    "        \n",
    "        embeddings = torch.cat((gcn_extracted_outputs, bert_embeddings), 1) \n",
    "        \n",
    "        return self.fc(embeddings)    \n",
    "    \n",
    "    \n",
    "class GPRModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.RGCN =  RGCNModel(h_dim = 1024, num_rels = 3, gated = True)\n",
    "#         self.BERThead = BERT_Head(1024) # bert output size\n",
    "        self.head = Head(256, 1024)  # gcn output   berthead output\n",
    "    \n",
    "    \n",
    "    def forward(self, g, offsets_gcn, cls_token):\n",
    "        gcn_outputs = self.RGCN(g)\n",
    "#         print(gcn_outputs.shape)\n",
    "        bert_head_outputs = cls_token\n",
    "        head_outputs = self.head(gcn_outputs, offsets_gcn, bert_head_outputs)\n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, graphs, gcn_offsets, cls_tokens, labels):\n",
    "\n",
    "        self.graphs = graphs\n",
    "        self.cls_tokens = cls_tokens\n",
    "        self.gcn_offsets = gcn_offsets\n",
    "        self.y = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.gcn_offsets[idx], self.cls_tokens[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GPRDataset(all_graphs, gcn_offsets, cls_tokens, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \n",
    "    graphs, gcn_offsets, cls_tokens, labels = map(list, zip(*samples))\n",
    "    \n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    offsets_gcn = torch.stack([torch.LongTensor(x) for x in gcn_offsets], dim=0)\n",
    "    \n",
    "    cls_tokens = torch.stack(cls_tokens, dim=0).squeeze()\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    \n",
    "    return batched_graph, offsets_gcn, cls_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "   train_dataset,\n",
    "   collate_fn = collate,\n",
    "   batch_size = 8,\n",
    "   shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 3.756\n",
      "[1,     2] loss: 3.821\n",
      "[1,     3] loss: 4.131\n",
      "[1,     4] loss: 4.159\n",
      "[1,     5] loss: 4.181\n",
      "[1,     6] loss: 4.826\n",
      "[1,     7] loss: 4.087\n",
      "[1,     8] loss: 3.873\n",
      "[1,     9] loss: 4.689\n",
      "[1,    10] loss: 4.476\n",
      "[1,    11] loss: 3.771\n",
      "[1,    12] loss: 3.963\n",
      "[1,    13] loss: 3.904\n",
      "[1,    14] loss: 4.542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'BinaryCrossEntropyWithLogitsBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-b56ce465168b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 nn.utils.clip_grad_norm_(model.RGCN.parameters(), 1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UCLA/spring_20/CS263/WinoGrande/.env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UCLA/spring_20/CS263/WinoGrande/.env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'BinaryCrossEntropyWithLogitsBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "model = GPRModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
    "\n",
    "reg_lambda = 0.035\n",
    "\n",
    "save_model_name = \"exp_model.pt\"\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        with autograd.detect_anomaly():\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "                graphs, gcn_offsets, cls_tokens, labels = data\n",
    "    #             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(graphs, gcn_offsets, cls_tokens)\n",
    "                \n",
    "#                 print(graphs, gcn_offsets, cls_tokens, labels)\n",
    "#                 print(outputs)\n",
    "\n",
    "    #             print(outputs, labels)\n",
    "\n",
    "                l2_reg = None\n",
    "                for w in model.RGCN.parameters():\n",
    "                    if not l2_reg:\n",
    "                        l2_reg = w.norm(2)\n",
    "                    else:\n",
    "                        l2_reg = l2_reg + w.norm(2)  \n",
    "                for w in model.head.parameters():\n",
    "                    if not l2_reg:\n",
    "                        l2_reg = w.norm(2)\n",
    "                    else:\n",
    "                        l2_reg = l2_reg + w.norm(2)\n",
    "\n",
    "                loss = criterion(outputs, labels) + l2_reg * reg_lambda\n",
    "                loss.backward()\n",
    "                \n",
    "#                 nn.utils.clip_grad_norm_(model.RGCN.parameters(), 1.0)\n",
    "#                 nn.utils.clip_grad_norm_(model.head.parameters(), 0.5)\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if i % 1 == 0:    # print every 20 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        torch.save(model, save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log of all things tried:\n",
    "1. Add autograd.detect_anomaly - saw that first outputs turn NaN then loss\n",
    "2. Comment out batch norm - _something_ changes - only first two rows of output are nan now...\n",
    "3. Increase batch size to 8 - no improvemnt\n",
    "4. tried setting gated = False for GPR model - no luck - in fact threw some error\n",
    "5.  Increasing dropout to 0.7 - un-helpful.\n",
    "6. clip_grad_norm was useless :(\n",
    "8. batch size 2 -> 56 iterations to failure\n",
    "\n",
    "\n",
    "    8.1 4 -> 28\n",
    "    8.2 8 -> 14\n",
    "    8.3 128 -> 0\n",
    "    \n",
    "9. Tried replacing Adam with SGD - no luck.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
